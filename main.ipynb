{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efa2cd4",
   "metadata": {},
   "source": [
    "# Face Verification ‚Äî Main (Jetson Nano compatible)\n",
    "\n",
    "This notebook provides a single runnable workflow that: \n",
    "\n",
    "- Captures an image from a local USB webcam (or falls back to Colab browser capture),\n",
    "- Loads a pretrained embedding model (TensorRT engine on Jetson, or Keras on other machines),\n",
    "- Builds a small gallery from reference images and runs a verification demo,\n",
    "- Runs a final gender-detection step using DeepFace (optional).\n",
    "\n",
    "Notes: On Jetson Nano prefer a TensorRT engine (`model.trt`). If you don't have one, the notebook will try to load `face_embedding_model_CLEAN.h5` (Keras). See `README_Jetson.md` for conversion steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install runtime packages if needed (uncomment when running interactively)\n",
    "# On Jetson, TensorRT & CUDA are provided by the system; avoid reinstalling them via pip.\n",
    "# The following installs are optional and may be skipped if already present.\n",
    "# !python3 -m pip install -r requirements.txt\n",
    "# For gender detection:\n",
    "# !pip install deepface --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bc2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project helpers (we added these to the repo)\n",
    "from camera_utils import capture_image\n",
    "\n",
    "# Try to import TensorRT helper; if not available we'll fall back to Keras\n",
    "TRT_AVAILABLE = False\n",
    "try:\n",
    "    from trt_inference import TRTInfer\n",
    "    import tensorrt as _trt  # noqa: F401\n",
    "    TRT_AVAILABLE = True\n",
    "except Exception:\n",
    "    TRT_AVAILABLE = False\n",
    "\n",
    "# Try to import tensorflow (only needed if falling back to Keras model)\n",
    "TF_AVAILABLE = False\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TF_AVAILABLE = True\n",
    "except Exception:\n",
    "    TF_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf590f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect platform heuristics (Jetson if tensorrt available or on aboard)\n",
    "def is_jetson():\n",
    "    # Simple heuristic: presence of TensorRT and CUDA on the system or the 'JETSON_SERIAL' env var\n",
    "    if TRT_AVAILABLE:\n",
    "        return True\n",
    "    if os.path.exists('/etc/nv_tegra_release') or os.path.exists('/usr/bin/jetson_release'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "ON_JETSON = is_jetson()\n",
    "print('TRT_AVAILABLE=', TRT_AVAILABLE, 'TF_AVAILABLE=', TF_AVAILABLE, 'ON_JETSON=', ON_JETSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL LOADING: prefer TensorRT engine (model.trt) on Jetson, else load Keras .h5 extractor\n",
    "ENGINE_PATH = 'model.trt'\n",
    "KERAS_H5 = 'face_embedding_model_CLEAN.h5'\n",
    "feature_extractor = None\n",
    "trt_infer = None\n",
    "\n",
    "if ON_JETSON and os.path.exists(ENGINE_PATH):\n",
    "    print('Loading TensorRT engine from', ENGINE_PATH)\n",
    "    try:\n",
    "        trt_infer = TRTInfer(ENGINE_PATH, input_shape=(128,128,3))\n",
    "        print('‚úÖ Loaded TRT engine.')\n",
    "    except Exception as e:\n",
    "        print('‚ùå TRT engine load failed:', e)\n",
    "        trt_infer = None\n",
    "\n",
    "if trt_infer is None:\n",
    "    # fallback to Keras model if available\n",
    "    if os.path.exists(KERAS_H5) and TF_AVAILABLE:\n",
    "        print('Loading Keras model from', KERAS_H5)\n",
    "        try:\n",
    "            feature_extractor = tf.keras.models.load_model(KERAS_H5, compile=False)\n",
    "            print('‚úÖ Loaded Keras feature extractor.')\n",
    "        except Exception as e:\n",
    "            print('‚ùå Failed to load Keras model:', e)\n",
    "    else:\n",
    "        print('No TRT engine or Keras model found. Please provide model.trt or face_embedding_model_CLEAN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and embedding helpers\n",
    "def preprocess_for_model(img, target_size=(128,128)):\n",
    "    # img: BGR (OpenCV) or RGB? We'll accept BGR and convert to RGB\n",
    "    if img is None:\n",
    "        return None\n",
    "    if img.shape[2] == 3:\n",
    "        # convert BGR->RGB\n",
    "        img = img[:, :, ::-1]\n",
    "    img = cv2.resize(img, (target_size[1], target_size[0]), interpolation=cv2.INTER_LINEAR)\n",
    "    img = img.astype('float32') / 255.0\n",
    "    return img\n",
    "\n",
    "def get_embedding_from_image(img):\n",
    "    # img: HxWxC BGR (from cv2.imread or capture), returns 1-D numpy vector\n",
    "    pre = preprocess_for_model(img, target_size=(128,128))\n",
    "    if pre is None:\n",
    "        return None\n",
    "    if trt_infer is not None:\n",
    "        return trt_infer.predict(pre)\n",
    "    if feature_extractor is not None:\n",
    "        # Keras expects batch dimension\n",
    "        inp = np.expand_dims(pre, axis=0)\n",
    "        emb = feature_extractor.predict(inp, verbose=0)[0]\n",
    "        return emb\n",
    "    raise RuntimeError('No model available for inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea110c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a small gallery from reference folder (if available)\n",
    "REF_DIR = './data_extracted/ref'\n",
    "gallery = {}\n",
    "if os.path.exists(REF_DIR):\n",
    "    for root, _, files in os.walk(REF_DIR):\n",
    "        for f in files:\n",
    "            if not f.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue\n",
    "            path = os.path.join(root, f)\n",
    "            try:\n",
    "                img = cv2.imread(path)\n",
    "                emb = get_embedding_from_image(img)\n",
    "                if emb is None:\n",
    "                    continue\n",
    "                pid = os.path.splitext(f)[0].split('__')[0]\n",
    "                # store first reference per person (simple demo)\n",
    "                if pid not in gallery:\n",
    "                    gallery[pid] = { 'path': path, 'emb': emb }\n",
    "            except Exception as e:\n",
    "                print('Error processing', path, e)\n",
    "\n",
    "print('Gallery size:', len(gallery))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19783a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple verification demo: capture a photo and compare to a random gallery entry\n",
    "if not gallery:\n",
    "    print('No gallery found; please populate', REF_DIR, 'and re-run this cell.')\n",
    "else:\n",
    "    # Select a reference entry\n",
    "    ref_pid, ref_data = next(iter(gallery.items()))\n",
    "    ref_path = ref_data['path']\n",
    "    print('Using reference:', ref_pid, ref_path)\n",
    "    # Capture a query image from local USB webcam\n",
    "    query_path = 'captured_query.jpg'\n",
    "    try:\n",
    "        capture_image(query_path, camera_index=0, timeout=5, interactive=False)\n",
    "        print('Captured', query_path)\n",
    "    except Exception as e:\n",
    "        print('Capture failed:', e)\n",
    "\n",
    "    # Load and embed\n",
    "    ref_img = cv2.imread(ref_path)\n",
    "    query_img = cv2.imread(query_path)\n",
    "    emb_ref = ref_data['emb']\n",
    "    emb_query = get_embedding_from_image(query_img)\n",
    "    dist = np.linalg.norm(emb_ref - emb_query)\n",
    "    threshold = 0.6\n",
    "    verdict = 'MATCH' if dist < threshold else 'REJECT'\n",
    "    print(f'Distance={dist:.4f} -> {verdict}')\n",
    "\n",
    "    # Show images side-by-side\n",
    "    ref_rgb = cv2.cvtColor(ref_img, cv2.COLOR_BGR2RGB)\n",
    "    query_rgb = cv2.cvtColor(query_img, cv2.COLOR_BGR2RGB)\n",
    "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "    ax[0].imshow(ref_rgb); ax[0].set_title(f'Ref: {ref_pid}'); ax[0].axis('off')\n",
    "    ax[1].imshow(query_rgb); ax[1].set_title(f'Query (D={dist:.4f})'); ax[1].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üß† GENDER DETECTION SECTION (FINAL FIXED VERSION)\n",
    "# =========================================================\n",
    "# Install deepface if needed (uncomment to run)\n",
    "# !pip install deepface --quiet\n",
    "\n",
    "from deepface import DeepFace\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_gender(image_path):\n",
    "    try:\n",
    "        # Clear previous TensorFlow models to avoid KerasTensor conflicts\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        # Run gender analysis safely\n",
    "        result = DeepFace.analyze(\n",
    "            img_path=image_path,\n",
    "            actions=['gender'],\n",
    "            enforce_detection=False\n",
    "        )\n",
    "        # DeepFace.analyze returns a dict for a single image in newer versions, or a list in some versions\n",
    "        if isinstance(result, list):\n",
    "            r = result[0]\n",
    "        else:\n",
    "            r = result\n",
    "        # `dominant_gender` is commonly returned; `gender` field may contain confidences\n",
    "        gender = r.get('dominant_gender', 'Unknown')\n",
    "        gender_conf_obj = r.get('gender', {})\n",
    "        # Try to extract confidence if available\n",
    "        confidence = 0.0\n",
    "        if isinstance(gender_conf_obj, dict):\n",
    "            confidence = gender_conf_obj.get(gender, 0.0) if gender in gender_conf_obj else list(gender_conf_obj.values())[0] if gender_conf_obj else 0.0\n",
    "    except Exception as e:\n",
    "        print(f'‚ö†Ô∏è Gender detection failed for {image_path}: {e}')\n",
    "        gender = 'Unknown'\n",
    "        confidence = 0.0\n",
    "    return gender, confidence\n",
    "\n",
    "# Use the reference and query paths from the previous verification step (if they exist)\n",
    "try:\n",
    "    ref_path  # noqa: F821\n",
    "except NameError:\n",
    "    # Fallback: pick any file from gallery if available\n",
    "    if gallery:\n",
    "        ref_path = next(iter(gallery.values()))['path']\n",
    "    else:\n",
    "        raise RuntimeError('No reference image available for gender detection')\n",
    "\n",
    "try:\n",
    "    query_path  # noqa: F821\n",
    "except NameError:\n",
    "    query_path = 'captured_query.jpg'\n",
    "\n",
    "# Run detection\n",
    "ref_gender, ref_conf = detect_gender(ref_path)\n",
    "query_gender, query_conf = detect_gender(query_path)\n",
    "\n",
    "# --- Display the images with detected genders ---\n",
    "ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)\n",
    "query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "fig.suptitle('Gender Detection Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "axes[0].imshow(ref_img_display)\n",
    "axes[0].set_title(f'Reference\\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(query_img_display)\n",
    "axes[1].set_title(f'Query\\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.9])\n",
    "plt.show()\n",
    "\n",
    "print('\n",
    "--- üß© GENDER DETECTION SUMMARY ---')\n",
    "print(f'‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)')\n",
    "print(f'‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aeb9a5",
   "metadata": {},
   "source": [
    "## Notes & Next Steps\n",
    "- If you plan to run on Jetson Nano, build the TensorRT engine on the Nano following `README_Jetson.md`.\n",
    "- If the notebook is run interactively, ensure the top cells that install packages are executed only once.\n",
    "- If you want, I can patch the original `Face_Verification.ipynb` to import from this consolidated `main.ipynb` flow or replace the duplicate capture/embedding cells across the notebook."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
